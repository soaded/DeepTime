{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soaded/DeepTime/blob/main/stable_diffusion_videos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4GhhH25OdYq"
      },
      "source": [
        "# Stable Diffusion Videos\n",
        "\n",
        "This notebook allows you to generate videos by interpolating the latent space of [Stable Diffusion](https://github.com/CompVis/stable-diffusion).\n",
        "\n",
        "You can either dream up different versions of the same prompt, or morph between different text prompts (with seeds set for each for reproducibility).\n",
        "\n",
        "If you like this notebook:\n",
        "- consider giving the [repo a star](https://github.com/nateraw/stable-diffusion-videos) â­ï¸\n",
        "- consider following me on Github [@nateraw](https://github.com/nateraw)\n",
        "\n",
        "You can file any issues/feature requests [here](https://github.com/nateraw/stable-diffusion-videos/issues)\n",
        "\n",
        "Enjoy ðŸ¤—"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable_diffusion_videos\n",
        "\n"
      ],
      "metadata": {
        "id": "QAcMsz6_8GqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvdCBpWWOhW-"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xwfc0ej1L9A0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796cd305-3b34-45b0-d81b-a1913d337672"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mBuilding wheel for tokenizers \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m No available output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for tokenizers (pyproject.toml) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31m  ERROR: Failed building wheel for tokenizers\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[1;31merror\u001b[0m: \u001b[1mfailed-wheel-build-for-install\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Failed to build installable wheels for some pyproject.toml based projects\n",
            "\u001b[31mâ•°â”€>\u001b[0m tokenizers\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Name: transformers\n",
            "Version: 5.2.0\n",
            "Summary: Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training.\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache 2.0 License\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: huggingface-hub, numpy, packaging, pyyaml, regex, safetensors, tokenizers, tqdm, typer-slim\n",
            "Required-by: peft, sentence-transformers, stable_diffusion_videos\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip setuptools -q\n",
        "\n",
        "# Uninstall potentially problematic existing versions\n",
        "!pip uninstall -y transformers diffusers tokenizers accelerate stable_diffusion_videos\n",
        "\n",
        "# Install specific versions in order to resolve build issues and version conflicts\n",
        "!pip install \"tokenizers==0.13.3\" -q\n",
        "!pip install \"transformers==4.25.1\" \"diffusers==0.12.1\" \"accelerate==0.15.0\" -q\n",
        "\n",
        "# Install the main library\n",
        "!pip install git+https://github.com/nateraw/stable-diffusion-videos -q\n",
        "\n",
        "# Verify the installed transformers version\n",
        "!pip show transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Consolidated package installations into cell Xwfc0ej1L9A0 to avoid conflicts."
      ],
      "metadata": {
        "id": "c9iGO9N-9KYP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7UOKJhVOonb"
      },
      "source": [
        "## Run the App ðŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g71hslP8OntM"
      },
      "source": [
        "### Load the Interface\n",
        "\n",
        "This step will take a couple minutes the first time you run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "bgSNS368L-DV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "e08da55f-6a22-4e10-81c5-5a8e02d40cca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'CLIPFeatureExtractor' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1756382933.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_diffusion_videos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_diffusion_videos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstable_diffusion_pipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionWalkPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m pipeline = StableDiffusionWalkPipeline.from_pretrained(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_diffusion_videos/stable_diffusion_pipeline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCLIPFeatureExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPTextModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCLIPTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mupsampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRealESRGANModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'CLIPFeatureExtractor' from 'transformers' (/usr/local/lib/python3.12/dist-packages/transformers/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "from stable_diffusion_videos.app import Interface\n",
        "from stable_diffusion_videos.stable_diffusion_pipeline import StableDiffusionWalkPipeline\n",
        "\n",
        "pipeline = StableDiffusionWalkPipeline.from_pretrained(\n",
        "    \"CompVis/stable-diffusion-v1-4\",\n",
        "    torch_dtype=torch.float16,\n",
        "    revision=\"fp16\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "interface = Interface(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kidtsR3c2P9Z"
      },
      "outputs": [],
      "source": [
        "#@title Connect to Google Drive to Save Outputs\n",
        "\n",
        "#@markdown If you want to connect Google Drive, click the checkbox below and run this cell. You'll be prompted to authenticate.\n",
        "\n",
        "#@markdown If you just want to save your outputs in this Colab session, don't worry about this cell\n",
        "\n",
        "connect_google_drive = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Then, in the interface, use this path as the `output` in the Video tab to save your videos to Google Drive:\n",
        "\n",
        "#@markdown > /content/gdrive/MyDrive/stable_diffusion_videos\n",
        "\n",
        "\n",
        "if connect_google_drive:\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxjRVNnMOtgU"
      },
      "source": [
        "### Launch\n",
        "\n",
        "This cell launches a Gradio Interface. Here's how I suggest you use it:\n",
        "\n",
        "1. Use the \"Images\" tab to generate images you like.\n",
        "    - Find two images you want to morph between\n",
        "    - These images should use the same settings (guidance scale, height, width)\n",
        "    - Keep track of the seeds/settings you used so you can reproduce them\n",
        "\n",
        "2. Generate videos using the \"Videos\" tab\n",
        "    - Using the images you found from the step above, provide the prompts/seeds you recorded\n",
        "    - Set the `num_interpolation_steps` - for testing you can use a small number like 3 or 5, but to get great results you'll want to use something larger (60-200 steps).\n",
        "\n",
        "ðŸ’¡ **Pro tip** - Click the link that looks like `https://<id-number>.gradio.app` below , and you'll be able to view it in full screen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8es3_onUOL3J"
      },
      "outputs": [],
      "source": [
        "interface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFCoTvlnPi4u"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjTQLCiLOWeo"
      },
      "source": [
        "## Use `walk` programmatically\n",
        "\n",
        "The other option is to not use the interface, and instead use `walk` programmatically. Here's how you would do that..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGQPClGwOR9R"
      },
      "source": [
        "First we define a helper fn for visualizing videos in colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqTWc8ZhNeLU"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def visualize_video_colab(video_path):\n",
        "    mp4 = open(video_path,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd_RzwkoPM7X"
      },
      "source": [
        "Walk! ðŸš¶â€â™€ï¸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv2wBZXXMQ-I"
      },
      "outputs": [],
      "source": [
        "video_path = pipeline.walk(\n",
        "    ['a cat', 'a dog'],\n",
        "    [42, 1337],\n",
        "    fps=5,                      # use 5 for testing, 25 or 30 for better quality\n",
        "    num_interpolation_steps=5,  # use 3-5 for testing, 30 or more for better results\n",
        "    height=512,                 # use multiples of 64 if > 512. Multiples of 8 if < 512.\n",
        "    width=512,                  # use multiples of 64 if > 512. Multiples of 8 if < 512.\n",
        ")\n",
        "visualize_video_colab(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLXULBMwSDnY"
      },
      "source": [
        "### Bonus! Music videos\n",
        "\n",
        "First, we'll need to install `youtube-dl`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install youtube-dl"
      ],
      "metadata": {
        "id": "302zMC44aiC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can download an example music file. Here we download one from my soundcloud:"
      ],
      "metadata": {
        "id": "Q3gCLCkLanzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! youtube-dl -f bestaudio --extract-audio --audio-format mp3 --audio-quality 0 -o \"music/thoughts.%(ext)s\" https://soundcloud.com/nateraw/thoughts"
      ],
      "metadata": {
        "id": "rEsTe_ujagE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "Audio(filename='music/thoughts.mp3')"
      ],
      "metadata": {
        "id": "RIKA-l5la28j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsIxXFTKSG5j"
      },
      "outputs": [],
      "source": [
        "# Seconds in the song\n",
        "audio_offsets = [7, 9]\n",
        "fps = 8\n",
        "\n",
        "# Convert seconds to frames\n",
        "num_interpolation_steps = [(b-a) * fps for a, b in zip(audio_offsets, audio_offsets[1:])]\n",
        "\n",
        "\n",
        "video_path = pipeline.walk(\n",
        "    prompts=['blueberry spaghetti', 'strawberry spaghetti'],\n",
        "    seeds=[42, 1337],\n",
        "    num_interpolation_steps=num_interpolation_steps,\n",
        "    height=512,                            # use multiples of 64\n",
        "    width=512,                             # use multiples of 64\n",
        "    audio_filepath='music/thoughts.mp3',    # Use your own file\n",
        "    audio_start_sec=audio_offsets[0],       # Start second of the provided audio\n",
        "    fps=fps,                               # important to set yourself based on the num_interpolation_steps you defined\n",
        "    batch_size=4,                          # increase until you go out of memory.\n",
        "    output_dir='./dreams',                 # Where images will be saved\n",
        "    name=None,                             # Subdir of output dir. will be timestamp by default\n",
        ")\n",
        "visualize_video_colab(video_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "7d7b96a25c39fa7937ff3ab94e1dd8c63b93cb924b8f0093093c6266e25a78bc"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}